{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73d30a8",
   "metadata": {},
   "source": [
    "# NLP, N-grams and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5e74f",
   "metadata": {},
   "source": [
    "As you have seen in the lectures, NLP has a wide range of techniques and applications of such techniques. We will give you an introduction to some of these techiques, and today you will get hands-on experience with them. In today's exercise, we will look at the following topics:\n",
    "\n",
    "1. How do we represent text in a vectorized way that encodes context? (One answer here is N-grams, and those we will look at).\n",
    "2. How do we create and sample from an N-gram language model - and how does the size of the grams affect the generated text?\n",
    "3. How do we use a pre-existing language model (FastText), to classify text messages as spam?\n",
    "\n",
    "The data we will be using later today is a dataset consisting of \"spam or ham\" text messages. The dataset consists of a number of text messages, some of which are spam and some of which are so-called \"ham\". We will use FastText to classify mails as spam or ham. For now, we will be looking at some different texts, to see how we can use N-grams to generate text, and how we can create N-gram language models from a text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256e6e7",
   "metadata": {},
   "source": [
    "## Exercise 1: Text-loading\n",
    "\n",
    "\n",
    "For now, the texts we will be experimenting with N-grams on, are the two famous books Pride & Prejudice by Jane Austen and The Origin of Species by Charles Darwin. The two books have been obtained in a raw text format from https://www.gutenberg.org/, i.e. Project Gutenberg which concerns itself with the collection of Open Access e-books.\n",
    "\n",
    "A big part of working with text documents is unfortunately having to preprocess the documents. Preprocessing of these, can have a large impact on the eventual performance of language models, such as N-gram models. We have included the text-preprocessing steps in the cell below. In the output cell you will notice that the first chapter of pride and prejudice is printed out. It is then preprocessed using the `preprocess_text` function and printed out again.\n",
    "\n",
    "* The preprocessing is not perfect. Do you notice any issues in the text? HINT: What happens to *good-humoured*? What happens to *three-and-twenty*? What happens to *Mr.* and *Mrs.*, and how will this later be handled when we split the sentences?"
   ]
  },
  {
   "cell_type": "code",
   "id": "22f31431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:08.772058Z",
     "start_time": "2024-10-22T13:15:08.632336Z"
    }
   },
   "source": [
    "import re\n",
    "import os\n",
    "import fasttext\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:08.788058Z",
     "start_time": "2024-10-22T13:15:08.779061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() #Lowercase everything in text file.\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.?! \\n]+\", \"\", text) #Remove unwanted special characters.\n",
    "    text = text.split(\"\\n\") #Split text by lines.\n",
    "    text = [line.strip() for line in text if line.find(\"chapter\") == -1] #Remove chapter headlines.\n",
    "    text = \"\\n\".join(text) #Recreate full document again\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"  \", \" \") #Remove end lines and remove double spacing.\n",
    "    return text"
   ],
   "id": "97712834817df01e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:09.701083Z",
     "start_time": "2024-10-22T13:15:09.640286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/pride_and_prejudice.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    pride_n_pred = file.read()\n",
    "    pride_n_pred_preproc = preprocess_text(pride_n_pred)"
   ],
   "id": "88535013892b3330",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ca75f97a",
   "metadata": {},
   "source": [
    "* Now load the Origin of Species text and preprocess as done to the Pride and Prejudice book above! You do not have to print the chapters out."
   ]
  },
  {
   "cell_type": "code",
   "id": "7fc1ea22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:09.823982Z",
     "start_time": "2024-10-22T13:15:09.765883Z"
    }
   },
   "source": [
    "with open(\"data/pride_and_prejudice.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    orig_of_spec = file.read()\n",
    "    orig_of_spec_preproc = preprocess_text(orig_of_spec)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "44a48b4f",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating N-grams\n",
    "\n",
    "Now that we have the texts in the preprocessed document format we want, we will move forward with the creation of our N-grams. Recall we want to use the N-grams for probabilistic word modelling tasks, for example, next word predictions given some sequence of words which we can express the following way:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1})\n",
    "\\end{equation}\n",
    "\n",
    "The problem is that estimating such probabilities for very long sequences is computationally and memory-wise VERY expensive. So as a solution we sometimes use N-grams. In N-grams, the assumption is that we can model these conditional dependencies with shorter sequences of words, i.e.:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n) & \\text{(Unigram)}\\\\\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n| w_{n-1}) & \\text{(Bigram)}\\\\\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n|w_{n-2}, w_{n-1}) & \\text{(Trigram)}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Which we then compute as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_n|w_{n-2}, w_{n-1}) = \\frac{\\text{Count}(w_{n-2}, w_{n-1}, w_n)}{\\text{Count}(w_{n-2}, w_{n-1})}\n",
    "\\end{equation}\n",
    "\n",
    "The language model that we create is based on some text corpus from which we obtain the count measures. In this exercise we will try making such N-gram models on the two books Origin of Species and Pride and Prejudice!\n",
    "\n",
    "In the cell below we have written the functions required for preprocessing a corpus even further such that it is ready for creating an N-gram model on. In order to guarantee that we can just start text generation or give conditional probabilites for how likely a start or end word is in given sentence we pad our sentence with start and end tokens denoted as `<s>` and `</s>` according to the size of N-grams we are working with.\n",
    "\n",
    "* Convince yourself why N-grams encode context in comparison to methods such as count vectorizers which just count words.\n",
    "* Make sure you understand the functions `tokenize_and_pad`, `n_gram` and `n_grams_to_prob_map`.\n",
    "* Try to vary the N-gram size N and inspect the first 20 N-grams. How do they change and why? Do you think there could be issues with this?"
   ]
  },
  {
   "cell_type": "code",
   "id": "476aa2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:11.069038Z",
     "start_time": "2024-10-22T13:15:09.857547Z"
    }
   },
   "source": [
    "def tokenize_and_pad(corpus, N=3):\n",
    "    corpus_sentences = corpus.split(\".\")\n",
    "    if N > 1:\n",
    "        padded_corpus_sentences = [\" \".join([\"<s>\"]*(N-1)) + \" \"+ sentence.strip() + \" \" + \" \".join([\"</s>\"]*(N-1)) for sentence in corpus_sentences]\n",
    "        tokenized_corpus_sentences = [[word for word in sentence.split(\" \")] for sentence in padded_corpus_sentences]\n",
    "    else:\n",
    "        tokenized_corpus_sentences = [[word for word in sentence.strip().split(\" \")] for sentence in corpus_sentences]\n",
    "    return tokenized_corpus_sentences\n",
    "\n",
    "def n_gram(tokenized_corpus_sentences, N=3):\n",
    "    n_grams_corpus = [zip(*[sentence[i:] for i in range(N)]) for sentence in tokenized_corpus_sentences]\n",
    "    n_grams = []\n",
    "    for n_grams_sentence in n_grams_corpus:\n",
    "        n_grams.extend([\" \".join(n_gram) for n_gram in n_grams_sentence])\n",
    "    return n_grams\n",
    "\n",
    "def n_grams_to_prob_map(n_grams):\n",
    "    contexts = {}\n",
    "    cond_prob = {}\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_split = n_gram.split(\" \")\n",
    "        context = \" \".join(n_gram_split[:N-1])\n",
    "        target = n_gram_split[N-1]\n",
    "        if context not in contexts.keys():\n",
    "            contexts[context] = {}\n",
    "            contexts[context][target] = 1\n",
    "        else:\n",
    "            if target in contexts[context].keys():\n",
    "                contexts[context][target] += 1\n",
    "            else:\n",
    "                contexts[context][target] = 1\n",
    "    for context in contexts.keys():\n",
    "        targets_count = [contexts[context][target] for target in contexts[context].keys()]\n",
    "        context_sum = np.sum(targets_count)\n",
    "        targets_prob = targets_count/context_sum\n",
    "        cond_prob[context] = (targets_prob, list(contexts[context].keys()))\n",
    "    return cond_prob\n",
    "N=3\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "orig_of_spec_n_grams = n_gram(orig_of_spec_tokenize, N=N)\n",
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "print(orig_of_spec_n_grams[:20])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> <s> it', '<s> it is', 'it is a', 'is a truth', 'a truth universally', 'truth universally acknowledged', 'universally acknowledged that', 'acknowledged that a', 'that a single', 'a single man', 'single man in', 'man in possession', 'in possession of', 'possession of a', 'of a good', 'a good fortune', 'good fortune must', 'fortune must be', 'must be in', 'be in want']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8e4e9e1a",
   "metadata": {},
   "source": [
    "## Exercise 3: Generating Text\n",
    "\n",
    "In the previous exercise we saw how to tokenize a corpus such that it is ready to make n-grams on. We then saw how to make n-grams and create a conditional probability based on these.\n",
    "\n",
    "The question now is, how can we generate a text using this conditional probability. A way of doing this is to sample from a conditional probability distribution based on our obtained N-grams. In essence, we can give a seed to our conditional probability (also called a context), and then we need to generate a word from our conditional probability by sampling from it.\n",
    "\n",
    "In the code below we have defined a function that allows us to generate a sentence based on a provided conditional distribution. In the cell we create such a conditional distribution and generate 5 sentences using the same text seed. Please note that the text-seed needs to be the size of the conditional variables, and this is ensured in the first 5 lines of the `generate_text` function!\n",
    "\n",
    "* Inspect the code below and try to understand what goes on in the `generate_text` function.\n",
    "* Why is it that even though we use the same text seed, the generated sentences changes?\n",
    "* What happens as you increase the N-gram size? Does this makes sense - and if so, why?\n",
    "* Is it more optimal to have smaller or larger N-gram size? Try to experiment with seeing generated sentences as N goes from 2->7.\n",
    "* What would it mean to set the N-gram size to one? What would you expect the generated text to look like?"
   ]
  },
  {
   "cell_type": "code",
   "id": "afbfa55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:12.285312Z",
     "start_time": "2024-10-22T13:15:11.097016Z"
    }
   },
   "source": [
    "def generate_text(cond_prob, text_seed, N, num_words=25):\n",
    "    generated_sentence = text_seed\n",
    "    if len(text_seed.split(\" \")) != N-1:\n",
    "        if len(text_seed.split(\" \")) < N-1:\n",
    "            text_seed = \" \".join([\"<s>\"]*(N-1-len(text_seed.split(\" \")))+text_seed.split(\" \"))\n",
    "        else:\n",
    "            text_seed = \" \".join(text_seed.split(\" \")[-N+1:]) #NOTE: Take ending words, not start sentences\n",
    "    context = text_seed\n",
    "    for i in range(num_words):\n",
    "        if context not in cond_prob.keys():\n",
    "            return generated_sentence\n",
    "        else:\n",
    "            generated_sentence += \" \" + np.random.choice(cond_prob[context][1], 1, p=cond_prob[context][0])[0]\n",
    "            context = \" \".join(generated_sentence.split(\" \")[-N+1:])\n",
    "    return generated_sentence\n",
    "\n",
    "\n",
    "\n",
    "N=3\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "orig_of_spec_n_grams = n_gram(orig_of_spec_tokenize, N=N)\n",
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "\n",
    "text_seed = \"it is said that instinct impels the cuckoo to\"\n",
    "\n",
    "for i in range(5):\n",
    "    print(generate_text(cond_prob=orig_of_spec_cond_prob, text_seed=text_seed, N=N) + \"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is said that instinct impels the cuckoo to\n",
      "\n",
      "it is said that instinct impels the cuckoo to\n",
      "\n",
      "it is said that instinct impels the cuckoo to\n",
      "\n",
      "it is said that instinct impels the cuckoo to\n",
      "\n",
      "it is said that instinct impels the cuckoo to\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "54164f68",
   "metadata": {},
   "source": [
    "We will now look at how the generated sentences changes depending on the corpus used to create our n-grams on.\n",
    "\n",
    "* Create N-grams and a conditional probability using the Pride and Prejudice corpus.\n",
    "* Try to generate some sentences using both conditional probabilites but using the same text seed (use ngram size 3 for example and use the provided text seed for both n-gram models. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "id": "81d7a6a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:13.513553Z",
     "start_time": "2024-10-22T13:15:12.350843Z"
    }
   },
   "source": [
    "#Write your code here for creating a sentence generator using Pride and Prejudice as your corpus\n",
    "#and comparing the two language models.\n",
    "N=3\n",
    "pride_n_pred_tokenize = tokenize_and_pad(pride_n_pred_preproc, N=N)\n",
    "pride_n_pred_n_grams = n_gram(pride_n_pred_tokenize, N=N)\n",
    "pride_n_pred_cond_prob = n_grams_to_prob_map(pride_n_pred_n_grams)\n",
    "\n",
    "text_seed = \"it is said that\"\n",
    "\n",
    "for i in range(5):\n",
    "    print(generate_text(cond_prob=pride_n_pred_cond_prob, text_seed=text_seed, N=N)+ \"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is said that he declared himself to produce a letter for her probably more than half a minute and then thought no more </s> </s>\n",
      "\n",
      "it is said that business with his family and friends </s> </s>\n",
      "\n",
      "it is said that she had never met with a call at the will of the latter of all </s> </s>\n",
      "\n",
      "it is said that i hardly know myself what it is a comfort to think that she talked on till they had been given up to nothing you could\n",
      "\n",
      "it is said that business was the case the want of importance which is to tempt anyone to our family party at pemberley mr </s> </s>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "321e3289",
   "metadata": {},
   "source": [
    "# FastText for Ham or Spam\n",
    "\n",
    "In the following exercises we will be looking at classifying text messages as \"Ham\" or \"Spam\" by using the FastText library. Recall that FastText is a library that allows us to train a language model easily to perform classification on other text pieces. In the following exercises we will:\n",
    "\n",
    "1. Load and split a dataset consisting of text messages with ham or spam text and accompanying labels.\n",
    "2. Train a FastText model to classify texts as ham or spam.\n",
    "3. Evaluate the FastText model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f29aa0",
   "metadata": {},
   "source": [
    "## Exercise 4: Loading spam or ham data\n",
    "\n",
    "In the following cell we use the pandas library to load our text delimited file which has the lables in the first column and the text messages in the second.\n",
    "\n",
    "* Use the `pandas.read_csv` function to read the `SMS_train.txt` file. Look up the documentation by googling. It may also require you to inspect the text file.\n",
    "* How many text messages are in the training set?\n",
    "* The `for` loop over the data is required for FastText as it expects a specific format for input files. Particularly, it wants a file which has the `__label__{label} text` layout in every line (where `__label__` is a token, i.e. something the FastText library reads as a keyword). Inspect the train_data.txt file to ensure you understand the format!"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:13.622226Z",
     "start_time": "2024-10-22T13:15:13.608190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_fasttext_format_txt(data_frame, path_to_doc):\n",
    "    texts = list(data_frame['1'])\n",
    "    labels = list(data_frame['0'])\n",
    "    txt = \"\"\n",
    "    for i, (label, text) in tqdm(enumerate(zip(labels, texts)), total=len(texts)):\n",
    "        txt = txt + f'__label__{label} {text}\\n'\n",
    "    \n",
    "    with open(path_to_doc, mode='w', encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "    return texts, labels"
   ],
   "id": "f1024b9656df3a1e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:13.891858Z",
     "start_time": "2024-10-22T13:15:13.720751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv(os.path.join(\"data\", \"SMS_train.txt\"))\n",
    "test_data = pd.read_csv('./data/SMS_test.txt', delimiter=',', encoding=\"utf-8\")\n",
    "\n",
    "display(train_data)\n",
    "\n",
    "train_texts, train_labels = create_fasttext_format_txt(data_frame=train_data, path_to_doc='data/train_data.txt')\n",
    "test_texts, test_labels = create_fasttext_format_txt(data_frame=test_data, path_to_doc='data/test_data.txt')"
   ],
   "id": "3004909b3d1703dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         0                                                  1\n",
       "0      ham                      Ok lar... Joking wif u oni...\n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "4      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "...    ...                                                ...\n",
       "4453   ham                                Ard 6 like dat lor.\n",
       "4454  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "4455  spam  This is the 2nd time we have tried 2 contact u...\n",
       "4456   ham  Pity, * was in mood for that. So...any other s...\n",
       "4457   ham                         Rofl. Its true to its name\n",
       "\n",
       "[4458 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows Ã— 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daede2217a594974b887399f1ae5f8c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1114 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f4d549a62e44a3e86e665eeb6018e4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "759df28b",
   "metadata": {},
   "source": [
    "## Exercise 5: Training a FastText Model \n",
    "\n",
    "We will now use the same dataset we just loaded to train a FastText model to perform classification. Remember, that in FastText, we not only have the option to create models that use word level N-grams, but also character level N-grams. We will try both and compare their performance!\n",
    "\n",
    "There are a number of parameters that can be passed to the FastText `train_supervised` function, but we will just concern ourselves with a couples of them.\n",
    "\n",
    "* The `input` parameter requires a text file as an input containing two columns. The first column must be the classification label and the second must be the text.\n",
    "* The `verbose` parameter just allows us to enable or disable training information. Here we enable it.\n",
    "* Now try to test the model using the `test` function. (HINT: See https://fasttext.cc/docs/en/supervised-tutorial.html). How good is your performance on the testset?\n",
    "* What happens when you vary the N-gram size? What is the optimal setting? Why do you think that is the case?\n",
    "* Look in the FastText documentation to find out how to make a character level model. Can you get better performance this way? Why do you think that is/isn't? \n",
    "* Try to tweak some of the parameters and look at what optimal parameter settings are. (HINT: Look at the `maxn` and `minn` parameters. If you find it difficult/annoying doing this manually, consider doing a hyperparameter search grid and find some optimal parameters!).\n",
    "* Try to preprocess the texts like we did in the previous exercise and see if this helps you!"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:14.064702Z",
     "start_time": "2024-10-22T13:15:14.046664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_fasttext_model(test_texts, test_labels, fasttext_model, verbose=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for text, label in zip(test_texts, test_labels):\n",
    "        prediction = fasttext_model.predict(text)[0][0]\n",
    "        if prediction == f'__label__{label}':\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    if verbose:\n",
    "        print(f'Word model accuracy: {accuracy * 100:.2f} %')\n",
    "    return accuracy"
   ],
   "id": "e2c29d7e69d66b3c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:15:15.441874Z",
     "start_time": "2024-10-22T13:15:14.216282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=3)\n",
    "accuracy_word_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_word_model, verbose=True)"
   ],
   "id": "6c0e63e9ecadb7d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word model accuracy: 95.87 %\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "c20d45ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:22:56.840166Z",
     "start_time": "2024-10-22T13:22:55.173192Z"
    }
   },
   "source": [
    "#Create char model here.\n",
    "char_gram_length_min = 3 # If set to zero, we only train word-grams\n",
    "char_gram_length_max = 6 # If set to zero, we only train word-grams\n",
    "\n",
    "fasttext_char_model = fasttext.train_supervised(\n",
    "    input='./data/train_data.txt',\n",
    "    verbose=True,\n",
    "     maxn=char_gram_length_max,\n",
    "    minn=char_gram_length_min\n",
    ")\n",
    "accuracy_char_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_char_model, verbose=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word model accuracy: 93.27 %\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
